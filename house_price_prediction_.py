# -*- coding: utf-8 -*-
"""HOUSE PRICE PREDICTION .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JeORWGJuOMlqPD7Y37yAzysVuGixiuBB
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as an

#suppress warnings for clean code
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("data.csv")

df.head(3)

#print number of unique values
df.nunique()

df.shape

for column in df:
  print(column)

#filtered dataset
col_to_select=[
'price',
'bedrooms',
'sqft_living',
'sqft_lot',
'floors',
'view',
'condition',
'sqft_above',
'yr_built',
'yr_renovated',
'city',
'country']

df=df[col_to_select]

df.shape

#how many null values
df.isna().sum()

#so, sqft_above has 2 missing values
col_to_add_zero=['sqft_above']
df[col_to_add_zero]=df[col_to_add_zero].fillna(0)#add zero to those missing places

#in bedroom let us add the mean values
df['bedrooms']=df['bedrooms'].fillna(df.bedrooms.mean())

#One-hot encoding
df=pd.get_dummies(df,drop_first=True)#drop_first=True is important to use, as it helps in reducing the extra column created during dummy variable creation
df.sample(3)

x=df.drop('price',axis=1)
y=df['price']

from sklearn.model_selection import train_test_split
train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=2)#random_state as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test. It produce the same results across a different run.

from sklearn.linear_model import LinearRegression
reg=LinearRegression()
reg.fit(train_x,train_y)

#test data accuracy
reg.score(test_x,test_y)

#train data accuracy
reg.score(train_x,train_y)

"""#this shows overfitting as, train_acc> test_acc"""

#use lasso regression which is L1 regularization to avoid overfitting
from sklearn.linear_model import Lasso
lasso_reg=Lasso(alpha=50, max_iter=100, tol=0.1)#Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. tol- is tolerance
lasso_reg.fit(train_x,train_y)

lasso_reg.score(test_x,test_y)

lasso_reg.score(train_x,train_y)

#to improve accuracy again and solve overfitting use L2 regularization or Ridge regression
from sklearn.linear_model import Ridge
ridge_reg=Ridge(alpha=50, max_iter=100, tol=0.1)
ridge_reg.fit(train_x,train_y)

ridge_reg.score(train_x,train_y)

ridge_reg.score(test_x,test_y)

"""##this shows that L2 regularization gives better accuracy than L1 and it solves the overfitting problem (train_acc> test_acc)"""

